{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(872, 30, 100) /n (873, 30, 100) /n (874, 30, 100) /n (873, 30, 100)\n",
      "(872, 30, 100) /n (872, 30, 100) /n (872, 30, 100) /n (872, 30, 100)\n"
     ]
    }
   ],
   "source": [
    "left_click_data  = np.load('dataset/seq_left_click_1646722489.npy')\n",
    "right_click_data = np.load('dataset/seq_right_click_1646722489.npy')\n",
    "Move_data        = np.load('dataset/seq_Move_1646722489.npy')\n",
    "None_data        = np.load('dataset/seq_None_1646722489.npy')\n",
    "\n",
    "print(f'{left_click_data.shape} /n {right_click_data.shape} /n {Move_data.shape} /n {None_data.shape}')\n",
    "\n",
    "right_click_data = np.delete(right_click_data, [872], axis=0) \n",
    "Move_data       = np.delete(Move_data , [873,872], axis=0) \n",
    "None_data       = np.delete(None_data , [872], axis=0) \n",
    "\n",
    "print(f'{left_click_data.shape} /n {right_click_data.shape} /n {Move_data.shape} /n {None_data.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3488, 30, 100)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions = ['left_click', 'right_click', 'Move', 'None']\n",
    "\n",
    "data = np.concatenate([\n",
    "    left_click_data,\n",
    "    right_click_data,\n",
    "    Move_data,\n",
    "    None_data\n",
    "], axis=0)\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3488, 30, 99)\n",
      "(3488,)\n"
     ]
    }
   ],
   "source": [
    "x_data = data[:, :, :-1]\n",
    "labels = data[:, 0, -1]\n",
    "\n",
    "print(x_data.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3488, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_data = to_categorical(labels, num_classes=len(actions))\n",
    "y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2441, 30, 99) (2441, 4)\n",
      "(1047, 30, 99) (1047, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_data = x_data.astype(np.float32)\n",
    "y_data = y_data.astype(np.float32)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.3, random_state=2022)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_2 (LSTM)               (None, 64)                41984     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 4)                 132       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44,196\n",
      "Trainable params: 44,196\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(64, activation='relu', input_shape=x_train.shape[1:3]),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(len(actions), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 13.0106 - acc: 0.5312\n",
      "Epoch 00001: val_acc improved from -inf to 0.78415, saving model to models\\model.h5\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 12.5332 - acc: 0.5408 - val_loss: 1.4435 - val_acc: 0.7841 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.8875 - acc: 0.8247\n",
      "Epoch 00002: val_acc improved from 0.78415 to 0.87775, saving model to models\\model.h5\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.8875 - acc: 0.8247 - val_loss: 0.5280 - val_acc: 0.8777 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 5.1257 - acc: 0.7988\n",
      "Epoch 00003: val_acc did not improve from 0.87775\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 5.0905 - acc: 0.7927 - val_loss: 4.6543 - val_acc: 0.5291 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 1.8269 - acc: 0.8325\n",
      "Epoch 00004: val_acc improved from 0.87775 to 0.92741, saving model to models\\model.h5\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 1.7566 - acc: 0.8394 - val_loss: 0.9800 - val_acc: 0.9274 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.7666 - acc: 0.9289\n",
      "Epoch 00005: val_acc improved from 0.92741 to 0.94842, saving model to models\\model.h5\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.7461 - acc: 0.9304 - val_loss: 0.3747 - val_acc: 0.9484 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.4096 - acc: 0.9366\n",
      "Epoch 00006: val_acc did not improve from 0.94842\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.3974 - acc: 0.9373 - val_loss: 0.2404 - val_acc: 0.9293 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 0.1881 - acc: 0.9459\n",
      "Epoch 00007: val_acc improved from 0.94842 to 0.96562, saving model to models\\model.h5\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.2079 - acc: 0.9463 - val_loss: 0.1241 - val_acc: 0.9656 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 0.1522 - acc: 0.9666\n",
      "Epoch 00008: val_acc improved from 0.96562 to 0.97230, saving model to models\\model.h5\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.1517 - acc: 0.9664 - val_loss: 0.0885 - val_acc: 0.9723 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 0.0843 - acc: 0.9859\n",
      "Epoch 00009: val_acc improved from 0.97230 to 0.98567, saving model to models\\model.h5\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.0885 - acc: 0.9853 - val_loss: 0.0668 - val_acc: 0.9857 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.0601 - acc: 0.9898\n",
      "Epoch 00010: val_acc improved from 0.98567 to 0.98758, saving model to models\\model.h5\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.0601 - acc: 0.9898 - val_loss: 0.0558 - val_acc: 0.9876 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 0.0484 - acc: 0.9887\n",
      "Epoch 00011: val_acc improved from 0.98758 to 0.99331, saving model to models\\model.h5\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.0476 - acc: 0.9889 - val_loss: 0.0305 - val_acc: 0.9933 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 0.0290 - acc: 0.9922\n",
      "Epoch 00012: val_acc did not improve from 0.99331\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.0427 - acc: 0.9918 - val_loss: 0.0248 - val_acc: 0.9933 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 0.0290 - acc: 0.9918\n",
      "Epoch 00013: val_acc did not improve from 0.99331\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.0280 - acc: 0.9922 - val_loss: 0.0255 - val_acc: 0.9924 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 0.0215 - acc: 0.9960\n",
      "Epoch 00014: val_acc improved from 0.99331 to 0.99713, saving model to models\\model.h5\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.0204 - acc: 0.9963 - val_loss: 0.0156 - val_acc: 0.9971 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "70/77 [==========================>...] - ETA: 0s - loss: 0.0115 - acc: 0.9978\n",
      "Epoch 00015: val_acc did not improve from 0.99713\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.0157 - acc: 0.9967 - val_loss: 0.0214 - val_acc: 0.9933 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 0.0193 - acc: 0.9960\n",
      "Epoch 00016: val_acc did not improve from 0.99713\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.0187 - acc: 0.9963 - val_loss: 0.0172 - val_acc: 0.9971 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "70/77 [==========================>...] - ETA: 0s - loss: 0.0152 - acc: 0.9964\n",
      "Epoch 00017: val_acc did not improve from 0.99713\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.0147 - acc: 0.9967 - val_loss: 0.0136 - val_acc: 0.9952 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 0.0093 - acc: 0.9987\n",
      "Epoch 00018: val_acc did not improve from 0.99713\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.0089 - acc: 0.9988 - val_loss: 0.0161 - val_acc: 0.9962 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 0.0081 - acc: 0.9982\n",
      "Epoch 00019: val_acc improved from 0.99713 to 0.99809, saving model to models\\model.h5\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.0078 - acc: 0.9984 - val_loss: 0.0458 - val_acc: 0.9981 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.0074 - acc: 0.9975\n",
      "Epoch 00020: val_acc did not improve from 0.99809\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.0074 - acc: 0.9975 - val_loss: 0.0441 - val_acc: 0.9981 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 0.0040 - acc: 0.9996\n",
      "Epoch 00021: val_acc did not improve from 0.99809\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.0040 - acc: 0.9996 - val_loss: 0.0418 - val_acc: 0.9971 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 0.0037 - acc: 0.9996\n",
      "Epoch 00022: val_acc improved from 0.99809 to 0.99904, saving model to models\\model.h5\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.0037 - acc: 0.9996 - val_loss: 0.0409 - val_acc: 0.9990 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "70/77 [==========================>...] - ETA: 0s - loss: 0.0036 - acc: 0.9996\n",
      "Epoch 00023: val_acc did not improve from 0.99904\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.0034 - acc: 0.9996 - val_loss: 0.0426 - val_acc: 0.9981 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 0.0029 - acc: 1.0000\n",
      "Epoch 00024: val_acc did not improve from 0.99904\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.0028 - acc: 1.0000 - val_loss: 0.0397 - val_acc: 0.9990 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 1.0000\n",
      "Epoch 00025: val_acc did not improve from 0.99904\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.0024 - acc: 1.0000 - val_loss: 0.0402 - val_acc: 0.9990 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 0.0124 - acc: 0.9982\n",
      "Epoch 00026: val_acc did not improve from 0.99904\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.0166 - acc: 0.9963 - val_loss: 0.0832 - val_acc: 0.9819 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "70/77 [==========================>...] - ETA: 0s - loss: 0.1245 - acc: 0.9826\n",
      "Epoch 00027: val_acc did not improve from 0.99904\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.1167 - acc: 0.9832 - val_loss: 0.0999 - val_acc: 0.9847 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 0.0875 - acc: 0.9844\n",
      "Epoch 00028: val_acc did not improve from 0.99904\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.0836 - acc: 0.9853 - val_loss: 0.0833 - val_acc: 0.9847 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 0.0123 - acc: 0.9974\n",
      "Epoch 00029: val_acc did not improve from 0.99904\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.0126 - acc: 0.9971 - val_loss: 0.0177 - val_acc: 0.9952 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/200\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 0.0116 - acc: 0.9969\n",
      "Epoch 00030: val_acc did not improve from 0.99904\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.0112 - acc: 0.9971 - val_loss: 0.0211 - val_acc: 0.9952 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 0.0033 - acc: 0.9996\n",
      "Epoch 00031: val_acc did not improve from 0.99904\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.0034 - acc: 0.9992 - val_loss: 0.0172 - val_acc: 0.9962 - lr: 0.0010\n",
      "Epoch 32/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 00032: val_acc did not improve from 0.99904\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.0135 - val_acc: 0.9990 - lr: 0.0010\n",
      "Epoch 33/200\n",
      "70/77 [==========================>...] - ETA: 0s - loss: 0.0132 - acc: 0.9987\n",
      "Epoch 00033: val_acc did not improve from 0.99904\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.0123 - acc: 0.9988 - val_loss: 0.0056 - val_acc: 0.9981 - lr: 0.0010\n",
      "Epoch 34/200\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 0.0025 - acc: 0.9996\n",
      "Epoch 00034: val_acc did not improve from 0.99904\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.0025 - acc: 0.9996 - val_loss: 0.0321 - val_acc: 0.9904 - lr: 0.0010\n",
      "Epoch 35/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 0.0057 - acc: 0.9983\n",
      "Epoch 00035: val_acc did not improve from 0.99904\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.0054 - acc: 0.9984 - val_loss: 0.0052 - val_acc: 0.9971 - lr: 0.0010\n",
      "Epoch 36/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 8.1517e-04 - acc: 1.0000\n",
      "Epoch 00036: val_acc did not improve from 0.99904\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 8.0725e-04 - acc: 1.0000 - val_loss: 0.0058 - val_acc: 0.9971 - lr: 0.0010\n",
      "Epoch 37/200\n",
      "70/77 [==========================>...] - ETA: 0s - loss: 7.1680e-04 - acc: 1.0000\n",
      "Epoch 00037: val_acc did not improve from 0.99904\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 7.0513e-04 - acc: 1.0000 - val_loss: 0.0048 - val_acc: 0.9971 - lr: 0.0010\n",
      "Epoch 38/200\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 5.6604e-04 - acc: 1.0000\n",
      "Epoch 00038: val_acc did not improve from 0.99904\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 5.9700e-04 - acc: 1.0000 - val_loss: 0.0036 - val_acc: 0.9971 - lr: 0.0010\n",
      "Epoch 39/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 5.2098e-04 - acc: 1.0000\n",
      "Epoch 00039: val_acc did not improve from 0.99904\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 5.2031e-04 - acc: 1.0000 - val_loss: 0.0027 - val_acc: 0.9990 - lr: 0.0010\n",
      "Epoch 40/200\n",
      "77/77 [==============================] - ETA: 0s - loss: 4.7184e-04 - acc: 1.0000\n",
      "Epoch 00040: val_acc did not improve from 0.99904\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 4.7184e-04 - acc: 1.0000 - val_loss: 0.0025 - val_acc: 0.9990 - lr: 0.0010\n",
      "Epoch 41/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 4.6738e-04 - acc: 1.0000\n",
      "Epoch 00041: val_acc did not improve from 0.99904\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 4.5442e-04 - acc: 1.0000 - val_loss: 0.0028 - val_acc: 0.9981 - lr: 0.0010\n",
      "Epoch 42/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 3.9670e-04 - acc: 1.0000\n",
      "Epoch 00042: val_acc did not improve from 0.99904\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 4.0308e-04 - acc: 1.0000 - val_loss: 0.0037 - val_acc: 0.9971 - lr: 0.0010\n",
      "Epoch 43/200\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 3.5162e-04 - acc: 1.0000\n",
      "Epoch 00043: val_acc did not improve from 0.99904\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 3.5828e-04 - acc: 1.0000 - val_loss: 0.0036 - val_acc: 0.9971 - lr: 0.0010\n",
      "Epoch 44/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 3.1509e-04 - acc: 1.0000\n",
      "Epoch 00044: val_acc did not improve from 0.99904\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 3.0675e-04 - acc: 1.0000 - val_loss: 0.0022 - val_acc: 0.9981 - lr: 0.0010\n",
      "Epoch 45/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 2.9822e-04 - acc: 1.0000\n",
      "Epoch 00045: val_acc did not improve from 0.99904\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 2.8442e-04 - acc: 1.0000 - val_loss: 0.0022 - val_acc: 0.9981 - lr: 0.0010\n",
      "Epoch 46/200\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 2.7766e-04 - acc: 1.0000\n",
      "Epoch 00046: val_acc did not improve from 0.99904\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 2.6237e-04 - acc: 1.0000 - val_loss: 0.0021 - val_acc: 0.9981 - lr: 0.0010\n",
      "Epoch 47/200\n",
      "70/77 [==========================>...] - ETA: 0s - loss: 2.2597e-04 - acc: 1.0000\n",
      "Epoch 00047: val_acc improved from 0.99904 to 1.00000, saving model to models\\model.h5\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 2.4614e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 48/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 2.1857e-04 - acc: 1.0000\n",
      "Epoch 00048: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 2.2620e-04 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 49/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 1.9693e-04 - acc: 1.0000\n",
      "Epoch 00049: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 2.0098e-04 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 50/200\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 1.8137e-04 - acc: 1.0000\n",
      "Epoch 00050: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 1.7465e-04 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 51/200\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 1.7656e-04 - acc: 1.0000\n",
      "Epoch 00051: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 1.7395e-04 - acc: 1.0000 - val_loss: 0.0021 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 52/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 1.6053e-04 - acc: 1.0000\n",
      "Epoch 00052: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 1.5341e-04 - acc: 1.0000 - val_loss: 0.0025 - val_acc: 0.9990 - lr: 0.0010\n",
      "Epoch 53/200\n",
      "76/77 [============================>.] - ETA: 0s - loss: 1.4420e-04 - acc: 1.0000\n",
      "Epoch 00053: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 1.4375e-04 - acc: 1.0000 - val_loss: 0.0026 - val_acc: 0.9990 - lr: 0.0010\n",
      "Epoch 54/200\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 1.2260e-04 - acc: 1.0000\n",
      "Epoch 00054: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 1.2876e-04 - acc: 1.0000 - val_loss: 0.0031 - val_acc: 0.9990 - lr: 0.0010\n",
      "Epoch 55/200\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 1.3696e-04 - acc: 1.0000\n",
      "Epoch 00055: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 1.2986e-04 - acc: 1.0000 - val_loss: 0.0033 - val_acc: 0.9990 - lr: 0.0010\n",
      "Epoch 56/200\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 1.0929e-04 - acc: 1.0000\n",
      "Epoch 00056: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 1.0804e-04 - acc: 1.0000 - val_loss: 0.0038 - val_acc: 0.9990 - lr: 0.0010\n",
      "Epoch 57/200\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 9.9278e-05 - acc: 1.0000\n",
      "Epoch 00057: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 1.0487e-04 - acc: 1.0000 - val_loss: 0.0044 - val_acc: 0.9990 - lr: 0.0010\n",
      "Epoch 58/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 9.9398e-05 - acc: 1.0000\n",
      "Epoch 00058: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 9.4853e-05 - acc: 1.0000 - val_loss: 0.0049 - val_acc: 0.9990 - lr: 0.0010\n",
      "Epoch 59/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/77 [===========================>..] - ETA: 0s - loss: 8.2894e-05 - acc: 1.0000\n",
      "Epoch 00059: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 8.8643e-05 - acc: 1.0000 - val_loss: 0.0055 - val_acc: 0.9990 - lr: 0.0010\n",
      "Epoch 60/200\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 8.7362e-05 - acc: 1.0000\n",
      "Epoch 00060: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 8.5205e-05 - acc: 1.0000 - val_loss: 0.0057 - val_acc: 0.9990 - lr: 0.0010\n",
      "Epoch 61/200\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 7.9377e-05 - acc: 1.0000\n",
      "Epoch 00061: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 7.6927e-05 - acc: 1.0000 - val_loss: 0.0066 - val_acc: 0.9990 - lr: 0.0010\n",
      "Epoch 62/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 3.4502e-04 - acc: 1.0000\n",
      "Epoch 00062: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 3.6087e-04 - acc: 1.0000 - val_loss: 0.0170 - val_acc: 0.9990 - lr: 0.0010\n",
      "Epoch 63/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 6.4534e-04 - acc: 1.0000\n",
      "Epoch 00063: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 6.2317e-04 - acc: 1.0000 - val_loss: 0.0164 - val_acc: 0.9990 - lr: 0.0010\n",
      "Epoch 64/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 3.2190e-04 - acc: 1.0000\n",
      "Epoch 00064: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 3.1930e-04 - acc: 1.0000 - val_loss: 0.0163 - val_acc: 0.9990 - lr: 0.0010\n",
      "Epoch 65/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 3.4288e-04 - acc: 1.0000\n",
      "Epoch 00065: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 3.3468e-04 - acc: 1.0000 - val_loss: 0.0170 - val_acc: 0.9990 - lr: 0.0010\n",
      "Epoch 66/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 2.1175e-04 - acc: 1.0000\n",
      "Epoch 00066: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 2.4473e-04 - acc: 1.0000 - val_loss: 0.0175 - val_acc: 0.9990 - lr: 0.0010\n",
      "Epoch 67/200\n",
      "77/77 [==============================] - ETA: 0s - loss: 2.1211e-04 - acc: 1.0000\n",
      "Epoch 00067: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 2.1211e-04 - acc: 1.0000 - val_loss: 0.0177 - val_acc: 0.9990 - lr: 0.0010\n",
      "Epoch 68/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 1.8864e-04 - acc: 1.0000\n",
      "Epoch 00068: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 1.8554e-04 - acc: 1.0000 - val_loss: 0.0187 - val_acc: 0.9990 - lr: 0.0010\n",
      "Epoch 69/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 1.7291e-04 - acc: 1.0000\n",
      "Epoch 00069: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 1.7670e-04 - acc: 1.0000 - val_loss: 0.0185 - val_acc: 0.9990 - lr: 0.0010\n",
      "Epoch 70/200\n",
      "70/77 [==========================>...] - ETA: 0s - loss: 1.5574e-04 - acc: 1.0000\n",
      "Epoch 00070: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 1.5378e-04 - acc: 1.0000 - val_loss: 0.0180 - val_acc: 0.9990 - lr: 0.0010\n",
      "Epoch 71/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 1.3797e-04 - acc: 1.0000\n",
      "Epoch 00071: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 1.3833e-04 - acc: 1.0000 - val_loss: 0.0188 - val_acc: 0.9990 - lr: 0.0010\n",
      "Epoch 72/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 1.3971e-04 - acc: 1.0000\n",
      "Epoch 00072: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 1.3772e-04 - acc: 1.0000 - val_loss: 0.0194 - val_acc: 0.9990 - lr: 0.0010\n",
      "Epoch 73/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 1.2266e-04 - acc: 1.0000\n",
      "Epoch 00073: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 1.1942e-04 - acc: 1.0000 - val_loss: 0.0187 - val_acc: 0.9990 - lr: 0.0010\n",
      "Epoch 74/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 1.1062e-04 - acc: 1.0000\n",
      "Epoch 00074: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 1.0785e-04 - acc: 1.0000 - val_loss: 0.0194 - val_acc: 0.9990 - lr: 0.0010\n",
      "Epoch 75/200\n",
      "70/77 [==========================>...] - ETA: 0s - loss: 9.7905e-05 - acc: 1.0000\n",
      "Epoch 00075: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 1.0014e-04 - acc: 1.0000 - val_loss: 0.0195 - val_acc: 0.9990 - lr: 0.0010\n",
      "Epoch 76/200\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 9.4877e-05 - acc: 1.0000\n",
      "Epoch 00076: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 9.5896e-05 - acc: 1.0000 - val_loss: 0.0193 - val_acc: 0.9990 - lr: 0.0010\n",
      "Epoch 77/200\n",
      "77/77 [==============================] - ETA: 0s - loss: 8.8351e-05 - acc: 1.0000\n",
      "Epoch 00077: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 8.8351e-05 - acc: 1.0000 - val_loss: 0.0186 - val_acc: 0.9990 - lr: 0.0010\n",
      "Epoch 78/200\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 8.3697e-05 - acc: 1.0000\n",
      "Epoch 00078: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 8.2656e-05 - acc: 1.0000 - val_loss: 0.0195 - val_acc: 0.9990 - lr: 0.0010\n",
      "Epoch 79/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 7.7685e-05 - acc: 1.0000\n",
      "Epoch 00079: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 7.4930e-05 - acc: 1.0000 - val_loss: 0.0197 - val_acc: 0.9990 - lr: 0.0010\n",
      "Epoch 80/200\n",
      "75/77 [============================>.] - ETA: 0s - loss: 6.7857e-05 - acc: 1.0000\n",
      "Epoch 00080: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 7.0366e-05 - acc: 1.0000 - val_loss: 0.0196 - val_acc: 0.9990 - lr: 0.0010\n",
      "Epoch 81/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 6.7847e-05 - acc: 1.0000\n",
      "Epoch 00081: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 6.7011e-05 - acc: 1.0000 - val_loss: 0.0023 - val_acc: 0.9990 - lr: 0.0010\n",
      "Epoch 82/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 6.1669e-05 - acc: 1.0000\n",
      "Epoch 00082: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 6.1651e-05 - acc: 1.0000 - val_loss: 0.0126 - val_acc: 0.9990 - lr: 0.0010\n",
      "Epoch 83/200\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 6.0903e-05 - acc: 1.0000\n",
      "Epoch 00083: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 5.8089e-05 - acc: 1.0000 - val_loss: 6.5490e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 84/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 5.4114e-05 - acc: 1.0000\n",
      "Epoch 00084: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 5.3924e-05 - acc: 1.0000 - val_loss: 0.0118 - val_acc: 0.9990 - lr: 0.0010\n",
      "Epoch 85/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 5.1251e-05 - acc: 1.0000\n",
      "Epoch 00085: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 4.9835e-05 - acc: 1.0000 - val_loss: 0.0103 - val_acc: 0.9990 - lr: 0.0010\n",
      "Epoch 86/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 4.6371e-05 - acc: 1.0000\n",
      "Epoch 00086: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 4.6832e-05 - acc: 1.0000 - val_loss: 0.0218 - val_acc: 0.9990 - lr: 0.0010\n",
      "Epoch 87/200\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 4.1636e-05 - acc: 1.0000\n",
      "Epoch 00087: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 4.3896e-05 - acc: 1.0000 - val_loss: 1.7437e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 88/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/77 [==========================>...] - ETA: 0s - loss: 4.1564e-05 - acc: 1.0000\n",
      "Epoch 00088: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 4.0959e-05 - acc: 1.0000 - val_loss: 1.8097e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 89/200\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 3.7614e-05 - acc: 1.0000\n",
      "Epoch 00089: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 3.7676e-05 - acc: 1.0000 - val_loss: 1.8080e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 90/200\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 3.4044e-05 - acc: 1.0000\n",
      "Epoch 00090: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 3.4893e-05 - acc: 1.0000 - val_loss: 1.6732e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 91/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 3.3575e-05 - acc: 1.0000\n",
      "Epoch 00091: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 3.3060e-05 - acc: 1.0000 - val_loss: 2.6235e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 92/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 3.0734e-05 - acc: 1.0000\n",
      "Epoch 00092: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 3.0907e-05 - acc: 1.0000 - val_loss: 1.7524e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 93/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 2.8589e-05 - acc: 1.0000\n",
      "Epoch 00093: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 2.8976e-05 - acc: 1.0000 - val_loss: 1.6336e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 94/200\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 2.7342e-05 - acc: 1.0000\n",
      "Epoch 00094: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 2.7280e-05 - acc: 1.0000 - val_loss: 1.6916e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 95/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 2.6331e-05 - acc: 1.0000\n",
      "Epoch 00095: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 2.5834e-05 - acc: 1.0000 - val_loss: 1.6055e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 96/200\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 2.3574e-05 - acc: 1.0000\n",
      "Epoch 00096: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 2.4545e-05 - acc: 1.0000 - val_loss: 1.6212e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 97/200\n",
      "70/77 [==========================>...] - ETA: 0s - loss: 2.2367e-05 - acc: 1.0000\n",
      "Epoch 00097: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 00097: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 2.3332e-05 - acc: 1.0000 - val_loss: 1.6691e-04 - val_acc: 1.0000 - lr: 0.0010\n",
      "Epoch 98/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 2.1505e-05 - acc: 1.0000\n",
      "Epoch 00098: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 2.1945e-05 - acc: 1.0000 - val_loss: 1.6175e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 99/200\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 2.1574e-05 - acc: 1.0000\n",
      "Epoch 00099: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 2.1279e-05 - acc: 1.0000 - val_loss: 1.5906e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 100/200\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 2.0841e-05 - acc: 1.0000\n",
      "Epoch 00100: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 2.0661e-05 - acc: 1.0000 - val_loss: 1.5897e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 101/200\n",
      "74/77 [===========================>..] - ETA: 0s - loss: 2.0484e-05 - acc: 1.0000\n",
      "Epoch 00101: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 2.0125e-05 - acc: 1.0000 - val_loss: 1.6102e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 102/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 1.9756e-05 - acc: 1.0000\n",
      "Epoch 00102: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 1.9620e-05 - acc: 1.0000 - val_loss: 1.5723e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 103/200\n",
      "70/77 [==========================>...] - ETA: 0s - loss: 1.9320e-05 - acc: 1.0000\n",
      "Epoch 00103: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 1.9015e-05 - acc: 1.0000 - val_loss: 1.5775e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 104/200\n",
      "70/77 [==========================>...] - ETA: 0s - loss: 1.8706e-05 - acc: 1.0000\n",
      "Epoch 00104: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 1.8575e-05 - acc: 1.0000 - val_loss: 1.5291e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 105/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 1.8291e-05 - acc: 1.0000\n",
      "Epoch 00105: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 1.8044e-05 - acc: 1.0000 - val_loss: 1.6013e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 106/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 1.5120e-05 - acc: 1.0000\n",
      "Epoch 00106: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 1.7522e-05 - acc: 1.0000 - val_loss: 1.5668e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 107/200\n",
      "70/77 [==========================>...] - ETA: 0s - loss: 1.7126e-05 - acc: 1.0000\n",
      "Epoch 00107: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 1.7088e-05 - acc: 1.0000 - val_loss: 1.5176e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 108/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 1.6902e-05 - acc: 1.0000\n",
      "Epoch 00108: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 1.6631e-05 - acc: 1.0000 - val_loss: 1.5382e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 109/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 1.5459e-05 - acc: 1.0000\n",
      "Epoch 00109: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 1.5996e-05 - acc: 1.0000 - val_loss: 1.5466e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 110/200\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 1.6072e-05 - acc: 1.0000\n",
      "Epoch 00110: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 1.5520e-05 - acc: 1.0000 - val_loss: 1.5254e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 111/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 1.5396e-05 - acc: 1.0000\n",
      "Epoch 00111: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 1.5093e-05 - acc: 1.0000 - val_loss: 1.5474e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 112/200\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 1.4425e-05 - acc: 1.0000\n",
      "Epoch 00112: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 1.4644e-05 - acc: 1.0000 - val_loss: 1.5800e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 113/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 1.4224e-05 - acc: 1.0000\n",
      "Epoch 00113: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 1.4277e-05 - acc: 1.0000 - val_loss: 1.5320e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 114/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 1.3072e-05 - acc: 1.0000\n",
      "Epoch 00114: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 1.3803e-05 - acc: 1.0000 - val_loss: 1.5547e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 115/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 1.3004e-05 - acc: 1.0000\n",
      "Epoch 00115: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 1.3459e-05 - acc: 1.0000 - val_loss: 1.5527e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 116/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/77 [==========================>...] - ETA: 0s - loss: 1.2453e-05 - acc: 1.0000\n",
      "Epoch 00116: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 1.3038e-05 - acc: 1.0000 - val_loss: 1.6153e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 117/200\n",
      "77/77 [==============================] - ETA: 0s - loss: 1.2625e-05 - acc: 1.0000\n",
      "Epoch 00117: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 1.2625e-05 - acc: 1.0000 - val_loss: 1.5740e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 118/200\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 1.2437e-05 - acc: 1.0000\n",
      "Epoch 00118: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 1.2232e-05 - acc: 1.0000 - val_loss: 1.5659e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 119/200\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 1.1721e-05 - acc: 1.0000\n",
      "Epoch 00119: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 1.1932e-05 - acc: 1.0000 - val_loss: 1.6086e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 120/200\n",
      "77/77 [==============================] - ETA: 0s - loss: 1.1493e-05 - acc: 1.0000\n",
      "Epoch 00120: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 1.1493e-05 - acc: 1.0000 - val_loss: 1.5938e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 121/200\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 1.1186e-05 - acc: 1.0000\n",
      "Epoch 00121: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 1.1185e-05 - acc: 1.0000 - val_loss: 1.5867e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 122/200\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 9.4091e-06 - acc: 1.0000\n",
      "Epoch 00122: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 1.0764e-05 - acc: 1.0000 - val_loss: 1.5908e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 123/200\n",
      "77/77 [==============================] - ETA: 0s - loss: 1.0548e-05 - acc: 1.0000\n",
      "Epoch 00123: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 1.0548e-05 - acc: 1.0000 - val_loss: 1.5006e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 124/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 1.0028e-05 - acc: 1.0000\n",
      "Epoch 00124: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 1.0150e-05 - acc: 1.0000 - val_loss: 1.5884e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 125/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 9.8172e-06 - acc: 1.0000\n",
      "Epoch 00125: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 9.8466e-06 - acc: 1.0000 - val_loss: 1.6064e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 126/200\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 9.0564e-06 - acc: 1.0000\n",
      "Epoch 00126: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 9.4928e-06 - acc: 1.0000 - val_loss: 1.6041e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 127/200\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 9.2175e-06 - acc: 1.0000\n",
      "Epoch 00127: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 9.2137e-06 - acc: 1.0000 - val_loss: 1.5844e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 128/200\n",
      "70/77 [==========================>...] - ETA: 0s - loss: 8.4433e-06 - acc: 1.0000\n",
      "Epoch 00128: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 8.9514e-06 - acc: 1.0000 - val_loss: 1.6342e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 129/200\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 8.7634e-06 - acc: 1.0000\n",
      "Epoch 00129: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 8.5965e-06 - acc: 1.0000 - val_loss: 1.5716e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 130/200\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 8.7019e-06 - acc: 1.0000\n",
      "Epoch 00130: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 8.3315e-06 - acc: 1.0000 - val_loss: 1.5700e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 131/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 8.3372e-06 - acc: 1.0000\n",
      "Epoch 00131: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 8.1092e-06 - acc: 1.0000 - val_loss: 1.5607e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 132/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 8.0026e-06 - acc: 1.0000\n",
      "Epoch 00132: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 7.8355e-06 - acc: 1.0000 - val_loss: 1.6166e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 133/200\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 7.8222e-06 - acc: 1.0000\n",
      "Epoch 00133: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 7.5900e-06 - acc: 1.0000 - val_loss: 1.5234e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 134/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 7.5761e-06 - acc: 1.0000\n",
      "Epoch 00134: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 7.3219e-06 - acc: 1.0000 - val_loss: 1.5169e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 135/200\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 7.1841e-06 - acc: 1.0000\n",
      "Epoch 00135: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 7.0888e-06 - acc: 1.0000 - val_loss: 1.5163e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 136/200\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 7.0140e-06 - acc: 1.0000\n",
      "Epoch 00136: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 6.8216e-06 - acc: 1.0000 - val_loss: 1.5431e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 137/200\n",
      "76/77 [============================>.] - ETA: 0s - loss: 6.6725e-06 - acc: 1.0000\n",
      "Epoch 00137: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 6.6544e-06 - acc: 1.0000 - val_loss: 1.4915e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 138/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 6.4890e-06 - acc: 1.0000\n",
      "Epoch 00138: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 6.3662e-06 - acc: 1.0000 - val_loss: 1.4297e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 139/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 6.1546e-06 - acc: 1.0000\n",
      "Epoch 00139: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 6.1402e-06 - acc: 1.0000 - val_loss: 1.3765e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 140/200\n",
      "70/77 [==========================>...] - ETA: 0s - loss: 5.7057e-06 - acc: 1.0000\n",
      "Epoch 00140: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 5.9236e-06 - acc: 1.0000 - val_loss: 1.3748e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 141/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 5.6956e-06 - acc: 1.0000\n",
      "Epoch 00141: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 5.7603e-06 - acc: 1.0000 - val_loss: 1.3520e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 142/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 5.9181e-06 - acc: 1.0000\n",
      "Epoch 00142: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 5.8861e-06 - acc: 1.0000 - val_loss: 1.3788e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 143/200\n",
      "70/77 [==========================>...] - ETA: 0s - loss: 5.0273e-06 - acc: 1.0000\n",
      "Epoch 00143: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 5.4274e-06 - acc: 1.0000 - val_loss: 1.3127e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 144/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/77 [===========================>..] - ETA: 0s - loss: 4.9913e-06 - acc: 1.0000\n",
      "Epoch 00144: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 5.1771e-06 - acc: 1.0000 - val_loss: 1.2275e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 145/200\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 4.7485e-06 - acc: 1.0000\n",
      "Epoch 00145: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 4.9233e-06 - acc: 1.0000 - val_loss: 1.1786e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 146/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 4.8288e-06 - acc: 1.0000\n",
      "Epoch 00146: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 4.7302e-06 - acc: 1.0000 - val_loss: 1.1532e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 147/200\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 4.6665e-06 - acc: 1.0000\n",
      "Epoch 00147: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 00147: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 4.5914e-06 - acc: 1.0000 - val_loss: 1.1396e-04 - val_acc: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 148/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 4.5536e-06 - acc: 1.0000\n",
      "Epoch 00148: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 4.4084e-06 - acc: 1.0000 - val_loss: 1.1393e-04 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 149/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 4.2932e-06 - acc: 1.0000\n",
      "Epoch 00149: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 4.3219e-06 - acc: 1.0000 - val_loss: 1.1238e-04 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 150/200\n",
      "75/77 [============================>.] - ETA: 0s - loss: 4.2761e-06 - acc: 1.0000\n",
      "Epoch 00150: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 4.2445e-06 - acc: 1.0000 - val_loss: 1.0904e-04 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 151/200\n",
      "70/77 [==========================>...] - ETA: 0s - loss: 3.9386e-06 - acc: 1.0000\n",
      "Epoch 00151: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 4.1807e-06 - acc: 1.0000 - val_loss: 1.0817e-04 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 152/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 3.9575e-06 - acc: 1.0000\n",
      "Epoch 00152: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 4.0758e-06 - acc: 1.0000 - val_loss: 1.0591e-04 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 153/200\n",
      "77/77 [==============================] - ETA: 0s - loss: 3.9994e-06 - acc: 1.0000\n",
      "Epoch 00153: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 3.9994e-06 - acc: 1.0000 - val_loss: 1.0427e-04 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 154/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 3.6349e-06 - acc: 1.0000\n",
      "Epoch 00154: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 3.9057e-06 - acc: 1.0000 - val_loss: 1.0281e-04 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 155/200\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 3.7798e-06 - acc: 1.0000\n",
      "Epoch 00155: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 3.8187e-06 - acc: 1.0000 - val_loss: 9.9535e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 156/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 3.6972e-06 - acc: 1.0000\n",
      "Epoch 00156: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 3.7391e-06 - acc: 1.0000 - val_loss: 9.8526e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 157/200\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 3.7679e-06 - acc: 1.0000\n",
      "Epoch 00157: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 3.6509e-06 - acc: 1.0000 - val_loss: 9.6657e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 158/200\n",
      "76/77 [============================>.] - ETA: 0s - loss: 3.5843e-06 - acc: 1.0000\n",
      "Epoch 00158: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 3.5743e-06 - acc: 1.0000 - val_loss: 9.2778e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 159/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 3.4863e-06 - acc: 1.0000\n",
      "Epoch 00159: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 3.5058e-06 - acc: 1.0000 - val_loss: 9.2229e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 160/200\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 3.5321e-06 - acc: 1.0000\n",
      "Epoch 00160: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 3.4280e-06 - acc: 1.0000 - val_loss: 9.0735e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 161/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 3.3311e-06 - acc: 1.0000\n",
      "Epoch 00161: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 3.3325e-06 - acc: 1.0000 - val_loss: 8.8734e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 162/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 3.3893e-06 - acc: 1.0000\n",
      "Epoch 00162: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 3.2594e-06 - acc: 1.0000 - val_loss: 8.6076e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 163/200\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 3.3401e-06 - acc: 1.0000\n",
      "Epoch 00163: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 3.1806e-06 - acc: 1.0000 - val_loss: 8.2912e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 164/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 3.0923e-06 - acc: 1.0000\n",
      "Epoch 00164: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 3.0903e-06 - acc: 1.0000 - val_loss: 8.2796e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 165/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 3.1101e-06 - acc: 1.0000\n",
      "Epoch 00165: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 3.0207e-06 - acc: 1.0000 - val_loss: 8.0635e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 166/200\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 2.9468e-06 - acc: 1.0000\n",
      "Epoch 00166: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 2.9368e-06 - acc: 1.0000 - val_loss: 7.8612e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 167/200\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 2.9202e-06 - acc: 1.0000\n",
      "Epoch 00167: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 2.8439e-06 - acc: 1.0000 - val_loss: 7.8615e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 168/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 2.7121e-06 - acc: 1.0000\n",
      "Epoch 00168: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 2.7703e-06 - acc: 1.0000 - val_loss: 7.5284e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 169/200\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 2.6757e-06 - acc: 1.0000\n",
      "Epoch 00169: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 2.7071e-06 - acc: 1.0000 - val_loss: 7.1640e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 170/200\n",
      "77/77 [==============================] - ETA: 0s - loss: 2.6208e-06 - acc: 1.0000\n",
      "Epoch 00170: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 2.6208e-06 - acc: 1.0000 - val_loss: 6.9468e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 171/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 2.6521e-06 - acc: 1.0000\n",
      "Epoch 00171: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 2.5607e-06 - acc: 1.0000 - val_loss: 6.9957e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 172/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/77 [============================>.] - ETA: 0s - loss: 2.4771e-06 - acc: 1.0000\n",
      "Epoch 00172: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 2.4680e-06 - acc: 1.0000 - val_loss: 6.6338e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 173/200\n",
      "70/77 [==========================>...] - ETA: 0s - loss: 2.3942e-06 - acc: 1.0000\n",
      "Epoch 00173: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 2.3925e-06 - acc: 1.0000 - val_loss: 6.4397e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 174/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 2.3611e-06 - acc: 1.0000\n",
      "Epoch 00174: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 2.3212e-06 - acc: 1.0000 - val_loss: 6.3137e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 175/200\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 2.2466e-06 - acc: 1.0000\n",
      "Epoch 00175: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 2.2377e-06 - acc: 1.0000 - val_loss: 6.1284e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 176/200\n",
      "76/77 [============================>.] - ETA: 0s - loss: 2.1759e-06 - acc: 1.0000\n",
      "Epoch 00176: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 2.1735e-06 - acc: 1.0000 - val_loss: 5.8686e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 177/200\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 2.0570e-06 - acc: 1.0000\n",
      "Epoch 00177: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 2.0860e-06 - acc: 1.0000 - val_loss: 5.6300e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 178/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 2.0714e-06 - acc: 1.0000\n",
      "Epoch 00178: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 2.0233e-06 - acc: 1.0000 - val_loss: 5.4135e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 179/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 1.9724e-06 - acc: 1.0000\n",
      "Epoch 00179: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 1.9415e-06 - acc: 1.0000 - val_loss: 5.3051e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 180/200\n",
      "77/77 [==============================] - ETA: 0s - loss: 1.8693e-06 - acc: 1.0000\n",
      "Epoch 00180: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 1.8693e-06 - acc: 1.0000 - val_loss: 5.4186e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 181/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 1.7664e-06 - acc: 1.0000\n",
      "Epoch 00181: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 1.7987e-06 - acc: 1.0000 - val_loss: 4.9525e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 182/200\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 1.6663e-06 - acc: 1.0000\n",
      "Epoch 00182: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 1.7262e-06 - acc: 1.0000 - val_loss: 4.8737e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 183/200\n",
      "70/77 [==========================>...] - ETA: 0s - loss: 1.5523e-06 - acc: 1.0000\n",
      "Epoch 00183: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 1.6562e-06 - acc: 1.0000 - val_loss: 4.5845e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 184/200\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 1.5343e-06 - acc: 1.0000\n",
      "Epoch 00184: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 1.5920e-06 - acc: 1.0000 - val_loss: 4.4762e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 185/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 1.5963e-06 - acc: 1.0000\n",
      "Epoch 00185: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 1.5341e-06 - acc: 1.0000 - val_loss: 4.5429e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 186/200\n",
      "77/77 [==============================] - ETA: 0s - loss: 1.4732e-06 - acc: 1.0000\n",
      "Epoch 00186: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 1.4732e-06 - acc: 1.0000 - val_loss: 4.1465e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 187/200\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 1.4840e-06 - acc: 1.0000\n",
      "Epoch 00187: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 1.4206e-06 - acc: 1.0000 - val_loss: 4.1015e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 188/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 1.3875e-06 - acc: 1.0000\n",
      "Epoch 00188: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 1.3605e-06 - acc: 1.0000 - val_loss: 4.0277e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 189/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 1.3291e-06 - acc: 1.0000\n",
      "Epoch 00189: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 1.3030e-06 - acc: 1.0000 - val_loss: 3.9057e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 190/200\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 1.2147e-06 - acc: 1.0000\n",
      "Epoch 00190: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 1.2471e-06 - acc: 1.0000 - val_loss: 3.6795e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 191/200\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 1.2057e-06 - acc: 1.0000\n",
      "Epoch 00191: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 1.2056e-06 - acc: 1.0000 - val_loss: 3.6519e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 192/200\n",
      "70/77 [==========================>...] - ETA: 0s - loss: 1.2035e-06 - acc: 1.0000\n",
      "Epoch 00192: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 1.1512e-06 - acc: 1.0000 - val_loss: 3.4542e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 193/200\n",
      "77/77 [==============================] - ETA: 0s - loss: 1.1074e-06 - acc: 1.0000\n",
      "Epoch 00193: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 1.1074e-06 - acc: 1.0000 - val_loss: 3.4169e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 194/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 1.0933e-06 - acc: 1.0000\n",
      "Epoch 00194: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 1.0627e-06 - acc: 1.0000 - val_loss: 3.3665e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 195/200\n",
      "72/77 [===========================>..] - ETA: 0s - loss: 9.9096e-07 - acc: 1.0000\n",
      "Epoch 00195: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 1.0252e-06 - acc: 1.0000 - val_loss: 3.5594e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 196/200\n",
      "77/77 [==============================] - ETA: 0s - loss: 9.7646e-07 - acc: 1.0000\n",
      "Epoch 00196: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 9.7646e-07 - acc: 1.0000 - val_loss: 3.2733e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 197/200\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 8.7039e-07 - acc: 1.0000\n",
      "Epoch 00197: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 00197: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 9.3764e-07 - acc: 1.0000 - val_loss: 3.3193e-05 - val_acc: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 198/200\n",
      "71/77 [==========================>...] - ETA: 0s - loss: 9.3230e-07 - acc: 1.0000\n",
      "Epoch 00198: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 9.0375e-07 - acc: 1.0000 - val_loss: 3.2108e-05 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 199/200\n",
      "73/77 [===========================>..] - ETA: 0s - loss: 8.8961e-07 - acc: 1.0000\n",
      "Epoch 00199: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 8.8543e-07 - acc: 1.0000 - val_loss: 3.2215e-05 - val_acc: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 200/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/77 [==========================>...] - ETA: 0s - loss: 8.1263e-07 - acc: 1.0000\n",
      "Epoch 00200: val_acc did not improve from 1.00000\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 8.6429e-07 - acc: 1.0000 - val_loss: 3.1119e-05 - val_acc: 1.0000 - lr: 1.2500e-04\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=200,\n",
    "    callbacks=[\n",
    "        ModelCheckpoint('models/model.h5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto'),\n",
    "        ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=50, verbose=1, mode='auto')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots(figsize=(16, 10))\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(history.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(history.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "loss_ax.legend(loc='upper left')\n",
    "\n",
    "acc_ax.plot(history.history['acc'], 'b', label='train acc')\n",
    "acc_ax.plot(history.history['val_acc'], 'g', label='val acc')\n",
    "acc_ax.set_ylabel('accuracy')\n",
    "acc_ax.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[791,   0],\n",
       "        [  0, 256]],\n",
       "\n",
       "       [[771,   0],\n",
       "        [  0, 276]],\n",
       "\n",
       "       [[784,   0],\n",
       "        [  0, 263]],\n",
       "\n",
       "       [[795,   0],\n",
       "        [  0, 252]]], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('models/model.h5')\n",
    "\n",
    "y_pred = model.predict(x_val)\n",
    "\n",
    "multilabel_confusion_matrix(np.argmax(y_val, axis=1), np.argmax(y_pred, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow] *",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
