{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as kl\n",
    "\n",
    "\n",
    "class ProbabilityDistribution(tf.keras.Model):\n",
    "  def call(self, logits, **kwargs):\n",
    "    # Sample a random categorical action from the given logits.\n",
    "    return tf.squeeze(tf.random.categorical(logits, 1), axis=-1)\n",
    "\n",
    "\n",
    "class Model(tf.keras.Model):\n",
    "  def __init__(self, num_actions):\n",
    "    super().__init__('mlp_policy')\n",
    "    # Note: no tf.get_variable(), just simple Keras API!\n",
    "    self.hidden1 = kl.Dense(128, activation='relu')\n",
    "    self.hidden2 = kl.Dense(128, activation='relu')\n",
    "    self.value = kl.Dense(1, name='value')\n",
    "    # Logits are unnormalized log probabilities.\n",
    "    self.logits = kl.Dense(num_actions, name='policy_logits')\n",
    "    self.dist = ProbabilityDistribution()\n",
    "\n",
    "  def call(self, inputs, **kwargs):\n",
    "    # Inputs is a numpy array, convert to a tensor.\n",
    "    x = tf.convert_to_tensor(inputs)\n",
    "    # Separate hidden layers from the same input tensor.\n",
    "    hidden_logs = self.hidden1(x)\n",
    "    hidden_vals = self.hidden2(x)\n",
    "    return self.logits(hidden_logs), self.value(hidden_vals)\n",
    "\n",
    "  def action_value(self, obs):\n",
    "    # Executes `call()` under the hood.\n",
    "    logits, value = self.predict_on_batch(obs)\n",
    "    action = self.dist.predict_on_batch(logits)\n",
    "    # Another way to sample actions:\n",
    "    #   action = tf.random.categorical(logits, 1)\n",
    "    # Will become clearer later why we don't use it.\n",
    "    return np.squeeze(action, axis=-1), np.squeeze(value, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.00015585]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "model = Model(num_actions=env.action_space.n)\n",
    "\n",
    "obs = env.reset()\n",
    "# No feed_dict or tf.Session() needed at all!\n",
    "action, value = model.action_value(obs[None, :])\n",
    "print(action, value) # [1] [-0.00145713]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 out of 200\n"
     ]
    }
   ],
   "source": [
    "agent = A2CAgent(model)\n",
    "rewards_sum = agent.test(env)\n",
    "print(\"%d out of 200\" % rewards_sum) # 18 out of 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.losses as kls\n",
    "import tensorflow.keras.optimizers as ko\n",
    "\n",
    "\n",
    "class A2CAgent:\n",
    "  def __init__(self, model, lr=7e-3, gamma=0.99, value_c=0.5, entropy_c=1e-4):\n",
    "    # Coefficients are used for the loss terms.\n",
    "    self.value_c = value_c\n",
    "    self.entropy_c = entropy_c\n",
    "    self.gamma = gamma\n",
    "    self.model = model\n",
    "    self.model.compile(\n",
    "      optimizer=ko.RMSprop(lr=lr),\n",
    "      # Define separate losses for policy logits and value estimate.\n",
    "      loss=[self._logits_loss, self._value_loss])\n",
    "\n",
    "  def test(self, env, render=True):\n",
    "    obs, done, ep_reward = env.reset(), False, 0\n",
    "    while not done:\n",
    "      action, _ = self.model.action_value(obs[None, :])\n",
    "      obs, reward, done, _ = env.step(action)\n",
    "      ep_reward += reward\n",
    "      if render:\n",
    "        env.render()\n",
    "    return ep_reward\n",
    "\n",
    " def train(self, env, batch_sz=64, updates=250):\n",
    "    # Storage helpers for a single batch of data.\n",
    "    actions = np.empty((batch_sz,), dtype=np.int32)\n",
    "    rewards, dones, values = np.empty((3, batch_sz))\n",
    "    observations = np.empty((batch_sz,) + env.observation_space.shape)\n",
    "\n",
    "    # Training loop: collect samples, send to optimizer, repeat updates times.\n",
    "    ep_rewards = [0.0]\n",
    "    next_obs = env.reset()\n",
    "    for update in range(updates):\n",
    "      for step in range(batch_sz):\n",
    "        observations[step] = next_obs.copy()\n",
    "        actions[step], values[step] = self.model.action_value(next_obs[None, :])\n",
    "        next_obs, rewards[step], dones[step], _ = env.step(actions[step])\n",
    "\n",
    "        ep_rewards[-1] += rewards[step]\n",
    "        if dones[step]:\n",
    "          ep_rewards.append(0.0)\n",
    "          next_obs = env.reset()\n",
    "          logging.info(\"Episode: %03d, Reward: %03d\" % (\n",
    "            len(ep_rewards) - 1, ep_rewards[-2]))\n",
    "\n",
    "      _, next_value = self.model.action_value(next_obs[None, :])\n",
    "\n",
    "      returns, advs = self._returns_advantages(rewards, dones, values, next_value)\n",
    "      # A trick to input actions and advantages through same API.\n",
    "      acts_and_advs = np.concatenate([actions[:, None], advs[:, None]], axis=-1)\n",
    "\n",
    "      # Performs a full training step on the collected batch.\n",
    "      # Note: no need to mess around with gradients, Keras API handles it.\n",
    "      losses = self.model.train_on_batch(observations, [acts_and_advs, returns])\n",
    "\n",
    "      logging.debug(\"[%d/%d] Losses: %s\" % (update + 1, updates, losses))\n",
    "\n",
    "    return ep_rewards\n",
    "\n",
    "  def _value_loss(self, returns, value):\n",
    "    # Value loss is typically MSE between value estimates and returns.\n",
    "    return self.value_c * kls.mean_squared_error(returns, value)\n",
    "\n",
    "  def _logits_loss(self, actions_and_advantages, logits):\n",
    "    # A trick to input actions and advantages through the same API.\n",
    "    actions, advantages = tf.split(actions_and_advantages, 2, axis=-1)\n",
    "\n",
    "    # Sparse categorical CE loss obj that supports sample_weight arg on `call()`.\n",
    "    # `from_logits` argument ensures transformation into normalized probabilities.\n",
    "    weighted_sparse_ce = kls.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    # Policy loss is defined by policy gradients, weighted by advantages.\n",
    "    # Note: we only calculate the loss on the actions we've actually taken.\n",
    "    actions = tf.cast(actions, tf.int32)\n",
    "    policy_loss = weighted_sparse_ce(actions, logits, sample_weight=advantages)\n",
    "\n",
    "    # Entropy loss can be calculated as cross-entropy over itself.\n",
    "    probs = tf.nn.softmax(logits)\n",
    "    entropy_loss = kls.categorical_crossentropy(probs, probs)\n",
    "\n",
    "    # We want to minimize policy and maximize entropy losses.\n",
    "    # Here signs are flipped because the optimizer minimizes.\n",
    "    return policy_loss - self.entropy_c * entropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
